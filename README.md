# Adversarial Vulnerability of Vision-Language Models (VLMs)

<p align="center">
  <img src="assets/thumbnail.jpg" alt="original and adversarial examples" style="max-width: 100%; height: auto;" />
  <em>Original and adversarial examples generated with PGD (PSNR = 43.53 dB) and C&W (PSNR = 50.18 dB)</em>
  <br><br>
</p>

## üîç Overview
This project demonstrates the vulnerability of Vision-Language Models (VLMs) under adversarial attacks. Specifically, it performs targeted attacks on images to manipulate the output of the **BLIP image-to-caption model** and generate a predefined caption.

---

## üéØ Objective
The goal is to apply adversarial perturbations to an image and feed it to the BLIP image-to-caption model, forcing the model to produce a specific, predetermined caption.

---

## üõ†Ô∏è Preparation

### 1. Create Conda Environment
```bash
conda create --name VLM python=3.8
conda activate VLM
```

### 2. Install PyTorch 1.13
```bash
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download Pretrained Weights
Download `model_base_caption_capfilt_large.pth` from here ([link](https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth)) and place it in the `pretrained_weights` folder.

---

## üìù Notebooks
Please run the following Jupyter notebooks in order.

1Ô∏è‚É£ [`Getting Familiar With BLIP.ipynb`](./1_Getting%20Familiar%20With%20BLIP.ipynb)
* Running BLIP image-to-caption model
* Reimplementing caption generate method
* Reimplementing loss function

2Ô∏è‚É£ [`Targeted FGSM (Linf).ipynb`](./2_Targeted%20FGSM%20(Linf).ipynb)
* Implementing Targeted Fast Gradient Sign Method (FGSM) with L‚àû
* Creating an adversarial example to change specific word in caption

3Ô∏è‚É£ [`Targeted Momentum PGD (Linf).ipynb`](./3_Targeted%20Momentum%20PGD%20(Linf).ipynb)
* Using Momentum Projected Gradient Descent (PGD) with L‚àû to find perturbations that force the model to produce a target caption
* Adding learning rate decay and random initialization to improve attack performance

4Ô∏è‚É£ [`Targeted C&W (L2).ipynb`](./4_Targeted%20C&W%20(L2).ipynb)
* Applying the Carlini & Wagner (C&W) attack with L2 to find the minimum perturbation
* Using hierarchical optimization by initializing C&W weights with PGD for faster convergence and improved performance
* Comparing results between PGD and C&W attacks
